# -*- coding: utf-8 -*-
"""Movie Recommendation System by MovieLens Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vARShK2u2rLYye0pGS3mPA5B8u9XyQAO

<h1> Submission Dicoding Movie Recommendation System by MovieLens Dataset </h1>

*   ```Nama Lengkap:``` **Rendika Nurhartanto Suharto**
*   ```Username:``` **rendika7**
*   ```Email:``` **rendikarendi96@gmail.com**
---

# **1. Library and Function Needed**

Tahap `Library and Function Needed` meliputi:

1. **File, Directory, and System Operations**: Digunakan untuk operasi dasar seperti mengakses file, menghitung waktu eksekusi, dan operasi matematika umum.
2. **Data Manipulation and Analysis**: Library untuk analisis dan manipulasi data dalam bentuk DataFrame, serta komputasi numerik seperti array dan perhitungan dasar.
3. **Data Visualization**: Digunakan untuk membuat visualisasi data baik statis maupun interaktif.
4. **Statistical and Data Analysis**: Fungsi untuk analisis statistik seperti uji distribusi atau regresi.
5. **Text Processing**: Konversi teks menjadi bentuk yang dapat digunakan oleh model pembelajaran mesin dan menghitung kesamaan antara teks.
6. **Natural Language Processing (NLP)**: Alat untuk pemrosesan teks lanjutan seperti stemming dan lemmatization.
7. **Recommendation Systems**: Membuat dan mengevaluasi sistem rekomendasi berbasis data pengguna.
8. **Warnings Suppression**: Mengabaikan peringatan yang muncul selama proses eksekusi agar tidak mengganggu output yang ditampilkan.
"""

!pip install surprise

# File, Directory, and System Operations ==========================
import os                     # Operasi file dan direktori
import time                   # Mengukur waktu
import math                   # Operasi matematika
import ast                    # Mengubah string menjadi tipe data asli (list, dict, dll.)

# Data Manipulation and Analysis ==========================
import pandas as pd            # Manipulasi dan analisis data (DataFrame)
import numpy as np             # Komputasi numerik (array, operasi matematis)
from collections import Counter # Menghitung jumlah objek dalam koleksi

# Data Visualization ==========================
import matplotlib.pyplot as plt # Visualisasi data (grafik, plot)
import seaborn as sns           # Visualisasi data (grafik yang lebih stylish)
import plotly.express as px     # Visualisasi interaktif

# Statistical and Data Analysis ==========================
from scipy import stats         # Analisis statistik (uji statistik, distribusi probabilitas)

# Text Processing ==========================
from ast import literal_eval    # Mengubah string menjadi tipe data asli (list, dict, dll.)
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Konversi teks ke vektor
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity # Menghitung kesamaan antar teks
from wordcloud import WordCloud  # Membuat wordcloud dari teks

# Natural Language Processing (NLP) ==========================
from nltk.stem.snowball import SnowballStemmer # Stemmer (menghapus akhiran kata)
from nltk.stem.wordnet import WordNetLemmatizer # Lemmatizer (mengembalikan kata ke bentuk dasar)
from nltk.corpus import wordnet                # Database leksikal untuk NLP

# Recommendation Systems ==========================
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate # Evaluasi model dengan cross-validation

# Warnings Suppression ==========================
import warnings
warnings.filterwarnings("ignore") # Mengabaikan peringatan selama eksekusi

"""**Fungsi Analisis dan Manipulasi Data**:
   - `check_duplicates()`: Memeriksa data duplikat dalam DataFrame.
   - `missing_data()`: Memeriksa nilai yang hilang.
   - `basic_data_info()`: Menampilkan informasi dasar, statistik deskriptif, dan visualisasi tipe data dari DataFrame.
"""

pd.set_option("display.max_columns", None) # Mengatur agar semua kolom ditampilkan
# pd.set_option("display.max_row", None)

def check_duplicates(dataframe, kolom=None):
    """
    Memeriksa nilai duplikat dari DataFrame.
    Menampilkan jumlah duplikat dan 10 nilai duplikat teratas (jika ada).
    Args:
        dataframe (pd.DataFrame): DataFrame yang akan diperiksa.
        kolom (list, optional): Kolom-kolom spesifik yang ingin diperiksa. Jika None, diperiksa semua kolom.
    """
    # Menampilkan nilai duplikat
    print("Nilai Duplikat (10 Teratas):")
    duplicate_values = dataframe[dataframe.duplicated(subset=kolom, keep=False)]
    duplicate_count = duplicate_values.shape[0]
    print(f"Jumlah Duplikat data: {duplicate_count}")
    if duplicate_count > 0:
        display(duplicate_values.head(10))
    else:
        print("Tidak ada duplikat yang ditemukan.")
    print("-" * 30)

def missing_data(data):
    """
    Memeriksa nilai yang hilang di DataFrame.
    Args:
        data (pd.DataFrame): DataFrame yang akan diperiksa.
    Returns:
        pd.DataFrame: DataFrame berisi total dan persentase nilai yang hilang.
    """
    total = data.isnull().sum().sort_values(ascending=False)
    percent = (data.isnull().sum() / data.isnull().count() * 100).sort_values(ascending=False).round(3)
    missing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

    print(f"\nTotal Kolom dengan Missing Values: {(total > 0).sum()}")
    print("-" * 30)

    return missing_df

def basic_data_info(dataframe):
    """
    Menampilkan informasi dasar dari DataFrame.
    Args:
        dataframe (pd.DataFrame): DataFrame yang akan ditampilkan informasinya.
    """
    # Menampilkan preview 5 baris pertama dari DataFrame
    print("Preview Data (5 Teratas):")
    print("-" * 30)
    display(dataframe.head())

    # Menampilkan info umum DataFrame
    print("\nInformasi Umum:")
    print("-" * 30)
    print(dataframe.info())

    # Menampilkan statistik deskriptif
    print("\nStatistik Deskriptif:")
    print("-" * 30)
    display(dataframe.describe().T)  # Transpose untuk lebih rapi

    # Menampilkan info tipe data unik di setiap kolom
    print("\nJumlah Nilai Unik per Kolom:")
    print("-" * 30)
    unique_counts = dataframe.nunique().sort_values(ascending=False)
    display(unique_counts)

    # Visualisasi distribusi tipe data
    plt.figure(figsize=(8, 5))
    sns.countplot(x=dataframe.dtypes, palette='Set2')
    plt.title("Distribusi Tipe Data")
    plt.xlabel("Tipe Data")
    plt.ylabel("Jumlah Kolom")
    plt.show()

"""# **2. Fetching Data From Kaggle**

1. **Mount Google Drive ke Colab**: Menghubungkan Google Drive agar file di Drive bisa diakses dari Colab.

2. **Set Konfigurasi Kaggle**: Mengatur variabel lingkungan untuk menunjukkan lokasi file `kaggle.json` yang berisi kredensial API Kaggle.

3. **Menentukan Lokasi Penyimpanan Dataset**: Menentukan folder di Google Drive untuk menyimpan dataset yang diunduh.

4. **Membuat Direktori Jika Belum Ada**: Membuat folder untuk dataset jika belum tersedia, agar dataset bisa disimpan.

5. **Mengunduh Dataset dari Kaggle**: Mengunduh dan mengekstrak dataset dari Kaggle ke folder yang sudah ditentukan di Google Drive.

Langkah-langkah ini memungkinkan pengunduhan dataset langsung dari Kaggle ke Google Drive melalui Colab.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Set-up Env Variable for Kaggle Config"""

# import os
# os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Colab Notebooks/9.2 DBS Foundation x Dicoding Kelas Expert 2024/Submission - Proyek Kedua/'

# # Tentukan path untuk menyimpan dataset
# path_to_save = '/content/drive/MyDrive/Colab Notebooks/9.2 DBS Foundation x Dicoding Kelas Expert 2024/Submission - Proyek Kedua/Dataset/'

# # Buat direktori jika belum ada
# os.makedirs(path_to_save, exist_ok=True)

"""## Download Dataset Using Kaggle Command"""

# # ## Unduh dataset menggunakan perintah kaggle
# !kaggle datasets download -d rounakbanik/the-movies-dataset --unzip -p '{path_to_save}'

"""# **2. Data Loading**

1. **Menentukan Path File**: Path utama dan folder dataset ditentukan terlebih dahulu agar file dapat diakses dengan mudah saat akan dibaca.

2. **Melihat Isi Direktori**: Anda dapat melihat file yang ada di dalam folder dataset untuk memastikan dataset yang diperlukan tersedia.

3. **Membaca Dataset**: Berbagai file CSV, seperti `movies_metadata.csv`, `links_small.csv`, `credits.csv`, `keywords.csv`, dan `ratings_small.csv`, di-load ke dalam DataFrame. Ini merupakan langkah untuk memuat data yang akan digunakan dalam analisis dan pengolahan.

4. **Membuat Pemetaan ID**: Membuat DataFrame khusus dari `links_small.csv` yang hanya memuat kolom `movieId` dan `tmdbId` untuk menghubungkan ID film antar file.
"""

mainPath = "/content/drive/MyDrive/Colab Notebooks/9.2 DBS Foundation x Dicoding Kelas Expert 2024/Submission - Proyek Kedua/"
dataPath = mainPath + "Dataset/"

# Change the current working directory
os.chdir(dataPath)

# Verify if the directory changed
!pwd  # This will show the current directory
!ls   # This will list files in the current directory

md = pd.read_csv(dataPath + "movies_metadata.csv")
md.head(2)

links_small = pd.read_csv(dataPath + 'links_small.csv')
links_small.head(2)

credits = pd.read_csv(dataPath + 'credits.csv')
keywords = pd.read_csv(dataPath + 'keywords.csv')

keywords.head(2)

credits.head(2)

ratings = pd.read_csv(dataPath + 'ratings_small.csv')
# ratings = pd.read_csv(dataPath + 'ratings.csv')

ratings.head(2)

id_map = pd.read_csv(dataPath + 'links_small.csv')[['movieId', 'tmdbId']]

# movies_metadata.csv, links_small.csv, credits.csv, keywords.csv, rating_small.csv

"""# **3. General Information in Dataset**

**Exploratory Data Analysis (EDA)** dengan fokus pada dataset-dataset yang relevan untuk proyek **Movie Recommendation System**. Berikut adalah penjelasan dari tiap bagian kode:

<h2> 3.1 movies_metadata.csv </h2>

1. **Memeriksa Dimensi Dataset (`md.shape`)**
   - Menampilkan ukuran (jumlah baris dan kolom) dari dataset `movies_metadata` untuk memberikan gambaran tentang seberapa besar dataset ini.

2. **Informasi Dasar Dataset (`basic_data_info(md)`)**
   - Fungsi ini menampilkan informasi ringkas seperti tipe data dari tiap kolom dan jumlah nilai yang hilang (missing values) pada dataset `movies_metadata`.

3. **Visualisasi Missing Values**
   - Membuat heatmap untuk memvisualisasikan distribusi missing values dalam dataset. Ini berguna untuk melihat sebaran kolom atau baris yang memiliki data yang hilang dengan mudah.

4. **Menghitung Nilai Unik (Uniqueness)**
   - `unique_values = md.nunique()` menghitung jumlah nilai unik di setiap kolom dalam dataset.
   - `unique_percentage = (unique_values / total_rows) * 100` menghitung persentase dari nilai unik tersebut terhadap jumlah total baris dalam dataset.
   - **Tujuannya**: Memahami variasi data di setiap kolom. Ini membantu untuk melihat kolom mana yang mungkin berguna atau tidak untuk analisis lebih lanjut.

5. **Filter Kolom dengan Nilai Unik Rendah**
   - Filter kolom yang memiliki jumlah nilai unik kecil (≤ 6). Ini seringkali berarti kolom tersebut memiliki data kategori yang terbatas, dan bisa digunakan untuk visualisasi lebih lanjut.
   
6. **Visualisasi Distribusi Kolom Terpilih**
   - Untuk setiap kolom yang terpilih, dibuat bar chart untuk melihat distribusi dari nilai-nilai yang ada. Nilai unik dari kolom tersebut akan ditampilkan di atas setiap batang bar chart untuk membantu pemahaman distribusi data.

<h2> 3.2 links_small.csv </h2>

1. **Memeriksa Dimensi Dataset (`links_small.shape`)**
   - Sama seperti pada `movies_metadata.csv`, langkah ini memeriksa ukuran dataset `links_small`.

2. **Informasi Dasar Dataset (`basic_data_info(links_small)`)**
   - Menampilkan tipe data dan jumlah missing values untuk dataset `links_small`.

<h2> 3.3 credits.csv </h2>

1. **Memeriksa Dimensi Dataset (`credits.shape`)**
   - Memeriksa ukuran dataset `credits`, yang berisi informasi tentang pemeran dan kru (cast and crew) dari setiap film.

2. **Informasi Dasar Dataset (`basic_data_info(credits)`)**
   - Menampilkan ringkasan tipe data dan missing values dari dataset `credits`.

<h2> 3.4 keywords.csv </h2>

1. **Memeriksa Dimensi Dataset (`keywords.shape`)**
   - Memeriksa ukuran dataset `keywords`, yang berisi kata kunci terkait dengan setiap film.

2. **Informasi Dasar Dataset (`basic_data_info(keywords)`)**
   - Sama seperti sebelumnya, menampilkan tipe data dan jumlah nilai yang hilang dalam dataset `keywords`.

3. **Analisis Kata Kunci (Keyword Analysis)**
   - **Langkah 1**: Kolom `keywords` sering kali disimpan dalam format JSON, sehingga menggunakan `ast.literal_eval()` untuk mengubahnya menjadi list Python yang bisa dianalisis.
   - **Langkah 2**: Membuat string yang berisi semua kata kunci dari seluruh film dan menggabungkannya menjadi satu string besar.
   - **Langkah 3**: Membuat visualisasi **WordCloud** untuk menampilkan kata kunci yang sering muncul dalam dataset. Ini membantu memahami tema umum dari film berdasarkan kata kunci yang digunakan.

<h2> 3.5 ratings_small.csv </h2>

1. **Memeriksa Dimensi Dataset (`ratings.shape`)**
   - Memeriksa ukuran dataset `ratings_small`, yang berisi informasi tentang rating yang diberikan pengguna pada film.

2. **Informasi Dasar Dataset (`basic_data_info(ratings)`)**
   - Sama seperti pada dataset sebelumnya, menampilkan tipe data dan missing values dari dataset `ratings_small`.

3. **Visualisasi Distribusi Rating**
   - **Bar Plot**: Menampilkan distribusi rating dalam bentuk bar chart untuk mengetahui berapa banyak pengguna yang memberikan nilai tertentu.
   - **KDE Plot**: Membuat visualisasi kernel density estimation (KDE) untuk menggambarkan distribusi rating secara lebih halus.

## **3.1 movies_metadata.csv**
"""

md.shape

basic_data_info(md)

# Membuat heatmap untuk melihat missing values
plt.figure(figsize=(12, 8))  # Mengatur ukuran plot
sns.heatmap(md.isnull(), cbar=False, cmap="viridis", yticklabels=False)

plt.title('Missing Data Heatmap')  # Menambahkan judul
plt.show()

# Menghitung nilai unik (uniqueness) untuk setiap kolom numerik
unique_values = md.nunique()

# Menghitung persentase uniqueness untuk setiap kolom
total_rows = len(md)
unique_percentage = (unique_values / total_rows) * 100

# Menyusun hasil dalam satu DataFrame
uniqueness_df = pd.DataFrame({
    'Unique Values': unique_values,
    'Percentage (%)': unique_percentage.round(2)  # Membulatkan hingga 2 desimal
})

# Filter kolom yang memiliki unique values kurang dari atau sama dengan 6
filtered_uniqueness_df = uniqueness_df[uniqueness_df['Unique Values'] <= 6]

# Menampilkan hasil
print("Kolom dengan Nilai Unik <= 6:")
display(filtered_uniqueness_df)

filtered_columns = filtered_uniqueness_df.index.tolist()

# Iterasi dan plot bar chart untuk setiap kolom yang difilter
for col in filtered_columns:
    plt.figure(figsize=(10, 6))
    column_value_counts = md[col].value_counts()  # Hitung jumlah nilai unik
    sns.barplot(x=column_value_counts.index, y=column_value_counts.values, palette='viridis')
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.title(f"Distribution of {col}")

    # Menampilkan jumlah di atas bar
    for i, v in enumerate(column_value_counts.values):
        plt.text(i, v + 0.5, str(v), ha='center', fontsize=12)

    # Memutar x-label agar lebih jelas (misal, 45 derajat atau 90 derajat)
    plt.xticks(rotation=45)  # Ubah rotation=90 jika ingin 90 derajat

    plt.show()

"""## **3.2 links_small.csv**"""

links_small.shape

basic_data_info(links_small)

"""## **3.3 credits.csv**"""

credits.shape

basic_data_info(credits)

"""## **3.4 keywords.csv**"""

keywords.shape

basic_data_info(keywords)

temp_ = keywords.copy()
temp_['keywords_count'] = temp_['keywords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])

all_keywords = ' '.join([keyword['name'] for keyword_list in temp_['keywords_count'] for keyword in keyword_list])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## **3.5 ratings_small.csv**"""

ratings.shape

basic_data_info(ratings)

plt.figure(figsize=(14, 6))

# Subplot 1: Bar plot of rating distribution
plt.subplot(1, 2, 1)
rating_counts = ratings['rating'].value_counts()
sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='viridis')
plt.xlabel("Ratings")
plt.ylabel("Count")
plt.title("Rating Distribution (Bar Plot)")

# Subplot 2: KDE plot of rating distribution
plt.subplot(1, 2, 2)
sns.kdeplot(ratings['rating'], color='blue', linewidth=2)
plt.xlabel("Ratings")
plt.ylabel("Density")
plt.title("Rating Distribution (KDE Plot)")

# Show the plot
plt.tight_layout()
plt.show()

"""# **4. MODELING**

## **4.1 Content Based Recommender**
"""

links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')
# Mengambil semua nilai dari kolom tmdbId yang tidak null, Kemudian mengonversi hasilnya menjadi tipe integer.

md = md.drop([19730, 29503, 35587]) # drop data yang di kolom adult isinya tidak sesaui dengan yang seharusnya

md['id'] = md['id'].astype('int') # Mengubah tipe data kolom 'id' menjadi integer

smd = md[md['id'].isin(links_small)]
smd.shape

# kode ini digunakan untuk menghasilkan DataFrame baru (smd) yang hanya berisi baris dari md di mana id sesuai dengan nilai yang terdapat di kolom tmdbId dari DataFrame links_small. Ini biasanya dilakukan untuk menyaring data berdasarkan kecocokan ID antara dua dataset.

"""### **4.1.1 Movie Description Based Recommender**

"""

smd['tagline'] = smd['tagline'].fillna('') # Mengganti semua nilai kosong (NaN) dalam kolom tagline dengan string kosong (''). Ini memastikan bahwa tidak ada nilai kosong dalam kolom tersebut, yang dapat mempermudah proses pengolahan data selanjutnya.
smd['description'] = smd['overview'] + smd['tagline'] # Membuat kolom baru yang bernama description dengan menggabungkan konten dari kolom overview dan tagline. Ini bertujuan untuk menciptakan deskripsi yang lebih lengkap untuk setiap film dengan menyertakan informasi dari kedua kolom tersebut.
smd['description'] = smd['description'].fillna('') # smd['description'] = smd['description'].fillna(''): Setelah penggabungan, kode ini mengganti nilai kosong (NaN) dalam kolom description dengan string kosong (''). Ini memastikan bahwa kolom description tidak memiliki nilai kosong, sehingga memudahkan dalam analisis atau visualisasi data selanjutnya.

"""menggunakan TfidfVectorizer untuk mengubah teks dalam kolom description dari DataFrame smd menjadi matriks TF-IDF. Berikut adalah penjelasan dari setiap komponen:

1. **Inisialisasi `TfidfVectorizer`**:
   - Mengatur parameter untuk analisis teks:
     - `analyzer='word'`: Menganalisis pada tingkat kata.
     - `ngram_range=(1, 2)`: Menghasilkan unigram (kata tunggal) dan bigram (dua kata berdampingan).
     - `min_df=0`: Memasukkan semua kata yang muncul setidaknya sekali.
     - `stop_words='english'`: Mengabaikan kata umum dalam bahasa Inggris.

2. **Menghitung Matriks TF-IDF**:
   - `tfidf_matrix = tf.fit_transform(smd['description'])`: Mengubah kolom `description` menjadi matriks TF-IDF, yang merepresentasikan pentingnya setiap kata dan bigram dalam konteks deskripsi film.

Dengan ini, teks deskripsi film telah diubah menjadi format numerik yang dapat digunakan untuk analisis lebih lanjut.
"""

tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2), min_df=0.0, stop_words='english')
tfidf_matrix = tf.fit_transform(smd['description'])

tfidf_matrix.shape

"""- Fungsi linear_kernel digunakan untuk menghitung kemiripan antara setiap pasangan vektor dalam matriks TF-IDF (tfidf_matrix).

- Hasilnya adalah matriks kemiripan kosinus (cosine_sim) yang menunjukkan seberapa mirip deskripsi film satu dengan yang lainnya. Nilai berkisar antara 0 (tidak mirip) hingga 1 (sangat mirip).
"""

cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

cosine_sim[0]

"""Sekarang kita memiliki matriks kesamaan kosinus berpasangan untuk semua film dalam kumpulan data kita. Langkah selanjutnya adalah menulis fungsi yang mengembalikan 30 film paling mirip berdasarkan skor kesamaan kosinus."""

# Menggabungkan genres dan tags menjadi satu kolom
smd['combined_clean'] = smd['tagline'] + ' | ' + smd['description']

# Mengatur ulang indeks DataFrame smd agar berurutan. dan Menyimpan kolom title ke dalam variabel titles.

smd = smd.reset_index()
titles = smd[['title', 'combined_clean']]
indices = pd.Series(smd.index, index=smd['title'])

def get_recommendations(movie_title, num_recommendations=10, cosine_sim=cosine_sim):
    # Cek apakah input adalah string
    if isinstance(movie_title, str):
        # Mengambil indeks film yang sesuai dengan judul (mengabaikan huruf besar/kecil)
        try:
            movie_index = smd[smd['title'].str.contains(movie_title, case=False, na=False)].index[0]
        except IndexError:
            print(f"Film dengan judul '{movie_title}' tidak ditemukan.")
            return None
    else:
        print("Identifier harus nama film dalam string!")
        return None

    # Menghitung skor kemiripan berdasarkan cosine similarity
    similarity_scores = list(enumerate(cosine_sim[movie_index]))  # Dapatkan skor kemiripan
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)  # Urutkan skor
    top_scores = similarity_scores[1:num_recommendations+1]  # Ambil rekomendasi sesuai jumlah yang diinginkan
    recommended_indices = [i[0] for i in top_scores]  # Ekstrak indeks film

    # Dapatkan film yang direkomendasikan
    recommendations = smd.iloc[recommended_indices][['title', 'combined_clean']]

    # Menghitung precision berdasarkan kemiripan genres dan tags
    target_tags = set(smd.loc[movie_index, 'combined_clean'].split())
    relevant_recommendations = recommendations[recommendations['combined_clean'].apply(lambda x: len(target_tags.intersection(set(x.split()))) > 0)]

    # Menghitung precision
    precision = len(relevant_recommendations) / num_recommendations

    print(f'Precision: {precision:.2f}\n')

    return recommendations.head(num_recommendations)

get_recommendations('The Godfather', num_recommendations=10)

get_recommendations('The Dark Knight', num_recommendations=10)

"""### **4.1.2 Metadata Based Content Recommender**

"""

# memastikan bahwa kolom id di tiga DataFrame (keywords, credits, dan md) memiliki tipe data yang sama, yaitu integer
keywords['id'] = keywords['id'].astype('int')
credits['id'] = credits['id'].astype('int')
md['id'] = md['id'].astype('int')

md.shape

# menggabungkan informasi dari dua DataFrame (credits dan keywords) ke dalam DataFrame md berdasarkan kolom id
md = md.merge(credits, on='id')
md = md.merge(keywords, on='id')

# menyaring DataFrame md untuk hanya menyertakan film yang juga terdapat dalam links_small
smd = md[md['id'].isin(links_small)]
smd.shape

'''
Menggunakan fungsi literal_eval untuk mengubah nilai-nilai dalam kolom cast, crew,
dan keywords dari string (teks) menjadi objek Python yang sesuai (list atau dictionary),
sehingga elemen-elemen di dalamnya bisa diakses dan dimanipulasi.
'''
smd['cast'] = smd['cast'].apply(literal_eval)
smd['crew'] = smd['crew'].apply(literal_eval)
smd['keywords'] = smd['keywords'].apply(literal_eval)

'''
Menghitung jumlah elemen di dalam cast dan crew untuk setiap film,
lalu menyimpan hasilnya di kolom baru, cast_size dan crew_size.
'''
smd['cast_size'] = smd['cast'].apply(lambda x: len(x))
smd['crew_size'] = smd['crew'].apply(lambda x: len(x))

# mengambil nama sutradara dari list anggota kru film. Jika tidak ada sutradara, fungsi akan mengembalikan nilai NaN.
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan

smd['director'] = smd['crew'].apply(get_director)

smd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else []) # Ambil nama aktor dari cast
smd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >=3 else x) # Batasi jumlah aktor yang diambil hingga 3

smd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else []) # Ambil nama dari keywords

smd.head(2)

"""Pendekatan dalam membangun sistem rekomendasi akan sangat sederhana. Rencana yang akan dilakukan adalah membuat dump metadata untuk setiap film yang terdiri dari genre, sutradara, aktor utama, dan kata kunci. Selanjutnya, Count Vectorizer akan digunakan untuk membuat matriks hitung seperti yang dilakukan pada Rekomendasi Deskripsi. Langkah-langkah berikutnya mirip dengan yang dilakukan sebelumnya: menghitung cosine similarities dan mengembalikan film-film yang paling mirip.

Langkah-langkah yang diikuti dalam mempersiapkan data genre dan kredit adalah:

1. Menghapus spasi dan mengubah semua fitur menjadi huruf kecil. Dengan cara ini, sistem tidak akan bingung antara Johnny Depp dan Johnny Galecki.
2. Menyebutkan sutradara sebanyak tiga kali untuk memberikan bobot lebih dibandingkan dengan seluruh anggota pemeran.
"""

smd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])

smd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(" ", "")))
smd['director'] = smd['director'].apply(lambda x: [x,x, x])

# Mengubah 'keywords' menjadi Series dan menghitung frekuensi kata kunci
keyword_series = smd['keywords'].explode().reset_index(drop=True)
keyword_counts = keyword_series.value_counts()

# Ambil hanya kata kunci yang muncul lebih dari sekali
frequent_keywords = keyword_counts[keyword_counts > 1]

frequent_keywords = frequent_keywords.value_counts()
frequent_keywords[:5]

frequent_keywords = frequent_keywords[frequent_keywords > 1]

# Menggunakan stemmer untuk mengubah kata menjadi bentuk dasarnya
stemmer = SnowballStemmer('english')
stemmed_example = stemmer.stem('dogs')

# Fungsi untuk memfilter kata kunci
def filter_keywords(x):
    return [word for word in x if word in frequent_keywords]

# Terapkan fungsi filter ke kolom 'keywords'
smd['keywords'] = smd['keywords'].apply(filter_keywords)
# Stemming kata kunci
smd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(word) for word in x])
# Menghapus spasi dari setiap kata dan mengubah menjadi huruf kecil
smd['keywords'] = smd['keywords'].apply(lambda x: [word.replace(" ", "").lower() for word in x])

smd[["cast", "genres"]]

# Mengubah 'director' menjadi list
smd['director'] = smd['director'].apply(lambda x: [x] if isinstance(x, str) else x)

# Mengambil nama dari genre
smd['genres'] = smd['genres'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])

# Membuat kolom 'combined_clean' dengan menggabungkan kolom yang relevan
smd['combined_clean'] = smd.apply(lambda x: x['keywords'] + x['cast'] + x['director'] + x['genres'], axis=1)

# Mengubah list dalam 'combined_clean' menjadi string
smd['combined_clean'] = smd['combined_clean'].apply(lambda x: ' '.join(x))

count = CountVectorizer(analyzer='word',ngram_range=(1, 2), min_df=0.0, stop_words='english')
count_matrix = count.fit_transform(smd['combined_clean'])

"""- Menggunakan CountVectorizer dari sklearn.feature_extraction.text untuk membuat matriks hitung dari kolom soup yang sudah dibentuk sebelumnya.

- Dengan matriks hitung ini, langkah selanjutnya umumnya adalah menghitung similarity antar film berdasarkan konten mereka. Matriks ini menjadi dasar untuk proses perhitungan kesamaan (seperti cosine similarity) yang akan digunakan dalam sistem rekomendasi film
"""

cosine_sim = cosine_similarity(count_matrix, count_matrix)

smd = smd.reset_index()
titles = smd[['title', 'combined_clean']]
indices = pd.Series(smd.index, index=smd['title'])

"""Fungsi `get_recommendations` yang telah ditulis sebelumnya akan digunakan kembali. Karena cosine similarity scores kita telah berubah, kita mengharapkan fungsi ini memberikan hasil yang berbeda (dan mungkin lebih baik). Mari kita periksa rekomendasi untuk film *The Dark Knight* sekali lagi dan lihat apa yang didapatkan kali ini."""

titles[titles['title'] == "The Dark Knight"]

get_recommendations('The Dark Knight', num_recommendations=10)

"""## **4.2 Colaborative Filtering**"""

print("Data rating:")
display(ratings.sample(2))

print("\nData film:")
display(smd.sample(2))

# Membuat objek Reader
reader = Reader()

# Memuat data dari DataFrame
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Melakukan cross-validation untuk mengevaluasi model
results = cross_validate(SVD(), data, measures=['RMSE', 'MAE'], cv=10, verbose=True)

# Menghitung rata-rata RMSE dan MAE
mean_rmse = results['test_rmse'].mean()
mean_mae = results['test_mae'].mean()

print(f"\nRata-rata RMSE: {mean_rmse:.4f}")
print(f"Rata-rata MAE: {mean_mae:.4f}")

# Membangun full trainset
trainset = data.build_full_trainset()

# Melatih model SVD
svd = SVD()
svd.fit(trainset)

# Menampilkan rating yang diberikan oleh user dengan userId tertentu
user_id = 1
user_ratings = ratings[ratings['userId'] == user_id]
print(f"\nRating yang diberikan oleh pengguna {user_id}:")
print(user_ratings)

# Memprediksi rating untuk film yang belum ditonton
all_movie_ids = ratings['movieId'].unique()
watched_movie_ids = user_ratings['movieId'].unique()
unwatched_movie_ids = [mid for mid in all_movie_ids if mid not in watched_movie_ids]

# Mengambil prediksi rating untuk film yang belum ditonton
predicted_ratings = []
for movie_id in unwatched_movie_ids:
    predicted_rating = svd.predict(user_id, movie_id).est
    predicted_ratings.append((movie_id, predicted_rating))

# Mengurutkan film berdasarkan rating yang diprediksi
predicted_ratings.sort(key=lambda x: x[1], reverse=True)

# Mengambil 10 rekomendasi film teratas
top_n = predicted_ratings[:10]
recommended_movie_ids = [mid for mid, _ in top_n]

# Mengambil detail film dari dataset
recommended_movies = smd[smd['id'].isin(recommended_movie_ids)][['original_title', 'vote_average', 'overview']]

# Menampilkan rekomendasi film sebagai DataFrame
recommended_movies_df = pd.DataFrame(recommended_movies)

# Menampilkan DataFrame hasil rekomendasi
print(f'\nTop 10 rekomendasi film untuk pengguna {user_id}:')
recommended_movies_df.reset_index(drop=True, inplace=True)  # Reset indeks agar lebih rapi
display(recommended_movies_df)